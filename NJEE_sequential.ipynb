{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook implements the entropy estimation of section 5.A in \n",
    "# https://arxiv.org/abs/2012.11197# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lGeBzmXZLaOI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import multinomial, geom, boltzmann, dlaplace\n",
    "from scipy.stats import poisson\n",
    "import os\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, TimeDistributed, LSTM\n",
    "from keras import optimizers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Simulated data generators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipf_dist(alpha, N, size):\n",
    "    x = np.arange(1, N+1, dtype='float')\n",
    "    weights = x ** (-alpha)\n",
    "    weights /= weights.sum()\n",
    "    bounded_zipf = stats.rv_discrete(name='bounded_zipf', values=(x, weights))\n",
    "    data = np.reshape(np.array(bounded_zipf.rvs(size=size)), [-1, 1])\n",
    "    return data, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipf_dist(alpha, N, size):\n",
    "    x = np.arange(1, N+1, dtype='float')\n",
    "    weights = x ** (-alpha)\n",
    "    weights /= weights.sum()\n",
    "    bounded_zipf = stats.rv_discrete(name='bounded_zipf', values=(x, weights))\n",
    "    data = np.reshape(np.array(bounded_zipf.rvs(size=size)), [-1, 1])\n",
    "    return data, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating H(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic(n, alpha):\n",
    "    a = 0\n",
    "    for i in range(1, n):\n",
    "        a += 1/i**alpha\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipf_entropy(alphabet, alpha):\n",
    "    p = np.arange(1, alphabet, dtype='float')**(-alpha)\n",
    "    c = harmonic(alphabet, alpha)\n",
    "    H_zipf = -(1/c)*np.sum(p*np.log(p/c))\n",
    "    return H_zipf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geom_entropy(weights):\n",
    "    return -np.sum(weights*np.log(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other utils\n",
    "def convert_to_one_hot(y, dict_size=None):\n",
    "    if dict_size is None:\n",
    "        dict_size = np.unique(y).shape[0]\n",
    "    y_hot = np.eye(dict_size)[y.astype('int32')]\n",
    "    return y_hot\n",
    "\n",
    "def make_one_hot(y, dims, dict_size=None):\n",
    "    y_hot = []\n",
    "    for i in range(dims):\n",
    "        y_hot.append(convert_to_one_hot(y[:, i], dict_size))\n",
    "    return y_hot\n",
    "\n",
    "def make_rnn_data(time_series, sequence_length):\n",
    "    seq_lst = []\n",
    "    time_series = np.array(time_series)\n",
    "    for i in range((len(time_series)-sequence_length+1)):\n",
    "        seq_lst.append(time_series[i:(i+sequence_length), :])\n",
    "    return np.array(seq_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6l8Hp4qXuCF"
   },
   "outputs": [],
   "source": [
    "def model_basic_classification(input_shape, class_size):\n",
    "    l0 = Input(shape=input_shape, dtype = 'float32', name = 'input_l')\n",
    "    X = LSTM(units=50, return_sequences=True, kernel_initializer='glorot_normal', name = 'l2')(l0)\n",
    "    output = TimeDistributed(Dense(class_size, activation='softmax'))(X)\n",
    "    model = Model(input = [l0], outputs =  output )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data from zipf, Geometric or mix \n",
    "source = 'zipf'\n",
    "alpha = 1\n",
    "alphabet = 10**5\n",
    "size = 1000\n",
    "p = 1/alphabet\n",
    "if source == 'zipf':\n",
    "    data, _ = zipf_dist(alpha, alphabet, size)\n",
    "    H_true = zipf_entropy(alphabet, alpha)\n",
    "elif source == 'geo':\n",
    "    data, weights = geom_dist(p, alphabet, size)\n",
    "    H_true = geom_entropy(weights)\n",
    "    \n",
    "# Convert symbols to binary representation\n",
    "dims = 25 # size of the binary representation, chosen by the dectated by the alphabet size\n",
    "vf = np.vectorize(np.binary_repr)\n",
    "data = vf(data, width=dims)\n",
    "lst = []\n",
    "for i in data:\n",
    "    lst.append([int(j) for j in i[0]])\n",
    "data = np.array(lst)\n",
    "\n",
    "# Prepare RNN input and output\n",
    "data_shift = np.hstack([data[:, 1:], np.zeros(shape=(len(data), 1))])\n",
    "data = np.expand_dims(data, axis=2)\n",
    "seq_lst = []\n",
    "for i in data_shift:\n",
    "    y = np.reshape(i, (-1, 1))\n",
    "    seq_lst.append(make_one_hot(y, 1, 2))\n",
    "data_shift = np.array(seq_lst)\n",
    "data_shift = np.squeeze(data_shift, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shale\\Anaconda3\\envs\\tf_1_15\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"ti..., inputs=[<tf.Tenso...)`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H estimated 8.014326745360242\n",
      "H true 7.967998491428407\n"
     ]
    }
   ],
   "source": [
    "seq_length = 25\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "bins = 2\n",
    "\n",
    "model = model_basic_classification([seq_length, 1], 2)\n",
    "opt = optimizers.Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
    "model.fit(data, data_shift, epochs=epochs, batch_size=batch_size, shuffle=True, verbose=0)\n",
    "CE = model.predict(data)*data_shift\n",
    "CE = np.mean(-np.log(CE.max(axis=2)).sum(axis=1))\n",
    "y = data[:, 0]\n",
    "p_1 = (np.sum(y)+10**-5)/float(size)\n",
    "H_1 = -np.sum(np.array([p_1, 1-p_1] )*np.log([p_1, 1-p_1])) # plug in estimator\n",
    "CE = CE + H_1\n",
    "\n",
    "print('H estimated', CE)\n",
    "print('H true', H_true)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "deep_TE20190813.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
