{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook implements the mutlinomial estimation of joint entropy using SInfoME building block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lGeBzmXZLaOI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras import optimizers\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from scipy.stats import random_correlation\n",
    "import scipy.stats as stats\n",
    "from entropy import *\n",
    "from scipy.stats import multinomial, geom\n",
    "from scipy.stats import poisson\n",
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import npeet package for KNN-based estimators\n",
    "sys.path.insert(0, '<path to NPEET package> \\\\NPEET-master\\\\npeet')\n",
    "import entropy_estimators as ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class to one hot \n",
    "def convert_to_one_hot(y, dict_size=None):\n",
    "    if dict_size is None:\n",
    "        dict_size = np.unique(y).shape[0]\n",
    "    y_hot = np.eye(dict_size)[y.astype('int32')]\n",
    "    return y_hot\n",
    "\n",
    "def make_one_hot(y, dims, dict_size=None):\n",
    "    y_hot = []\n",
    "    for i in range(dims):\n",
    "        y_hot.append(convert_to_one_hot(y[:, i], dict_size))\n",
    "    return y_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6l8Hp4qXuCF"
   },
   "outputs": [],
   "source": [
    "# Basic fully connected model with number of losses equales to dims - the dimensions of Y\n",
    "def model_basic_classification(input_shape, class_size):\n",
    "    l0 = Input(shape=input_shape, dtype = 'float32', name = 'input_l')\n",
    "    X = Dense(units=500, kernel_initializer='random_uniform', name = 'l1')(l0)\n",
    "    X = Dense(units=500, kernel_initializer='random_uniform', name = 'l2')(X)\n",
    "\n",
    "\n",
    "    output = [Dense(class_size, activation='softmax')(X) for i in range(1) ]\n",
    "    model = Model(input = [l0], outputs =  output )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating H(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training paramteters\n",
    "epochs = 10\n",
    "batch_size = 512\n",
    "\n",
    "# lists that will obtain the entropies estimations\n",
    "sub_loss_lst = []\n",
    "H_y_lst = [ [] for _ in range(dims)]\n",
    "H_yx_lst = [ [] for _ in range(dims)]\n",
    "H_y_total = []\n",
    "results = []\n",
    "\n",
    "# KNN based lists\n",
    "H_sota_1 = []\n",
    "H_sota_3 = []\n",
    "H_sota_5 = []\n",
    "H_sota_10 = []\n",
    "H_sota_20 = []\n",
    "hist_lst = []\n",
    "\n",
    "# Will be used to convert KNN estimates from bits to nats\n",
    "scale = np.log2(np.exp(1))\n",
    "\n",
    "# parameters for simulated data and entropy true value of multinomial entropy given the known values of p\n",
    "length = 1000 #dataset size\n",
    "p = np.random.uniform(size=dims) #multinomial\n",
    "p /= p.sum()\n",
    "H_true = multinomial.entropy(n=100, p=p)\n",
    "\n",
    "\n",
    "n = np.repeat(length, 100) #number of experiments\n",
    "for k, size in enumerate(n):    \n",
    "    \n",
    "    # Generating multinomial data\n",
    "    data = multinomial.rvs(n=100, p=p,size=size)\n",
    "    data = np.array(data)\n",
    "    \n",
    "    # The number of classes - the maximum value obtained from data + 1 (including zero)\n",
    "    class_size = data.max() + 1\n",
    "    \n",
    "    # Estimating entropy with the KNN estimator\n",
    "    H_y_lst = [ [] for _ in range(dims)]\n",
    "    H_sota_1.append(ee.entropy(data, k=1)/scale)\n",
    "    H_sota_3.append(ee.entropy(data, k=3)/scale)\n",
    "    H_sota_5.append(ee.entropy(data, k=5)/scale)\n",
    "    H_sota_10.append(ee.entropy(data, k=10)/scale)\n",
    "    H_sota_20.append(ee.entropy(data, k=20)/scale)\n",
    "    \n",
    "    \n",
    "    \n",
    "   # Preparing m - 1 NN models\n",
    "    model_lst = []\n",
    "    opt_lst = []\n",
    "    for m in range(0, dims):\n",
    "        if m == 0:\n",
    "            model_lst.append(None)\n",
    "        else:\n",
    "            model_lst.append(model_basic_classification([m], class_size))\n",
    "\n",
    "    for m in range(0, dims):\n",
    "        if m == 0:\n",
    "            opt_lst.append(None)\n",
    "        else:\n",
    "            opt_lst.append(optimizers.Adam())\n",
    "\n",
    "    # Apply SInfoME block m - 1 times\n",
    "    for m in range (1, dims):\n",
    "        model_lst[m].compile(loss='categorical_crossentropy', optimizer=opt_lst[m], metrics=['accuracy'])\n",
    "    \n",
    "    for j in range(0, dims):\n",
    "        print('dim number', j)\n",
    "        if j != 0:\n",
    "            x = data[:, range(j)]\n",
    "            y = data[:, j]\n",
    "            y = np.reshape(y, [-1, 1])\n",
    "            y_hot = make_one_hot(y, 1, class_size)\n",
    "            hist = model_lst[j].fit(x, y_hot, epochs=epochs, batch_size=batch_size, validation_split=0.3, verbose=0)\n",
    "            H_y_lst[j].append(hist.history['loss'])\n",
    "        else:\n",
    "            y = data[:, j]\n",
    "            _, p_1 = np.unique(y, return_counts=True)\n",
    "            p_1 = p_1/(p_1.sum() + 10**-5) \n",
    "        \n",
    "            H_y_lst[j].append([np.sum(np.array(p_1)*np.log(p_1))]*epochs)\n",
    "    plt.plot(hist.history['loss'])\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.show()\n",
    "\n",
    "    H_y_total.append(np.reshape(np.sum(H_y_lst, axis=0, ), [-1]))\n",
    "    results.append(np.reshape(H_y_total, [-1])[-1])\n",
    "    \n",
    "print('Results from SInfoME:', results) \n",
    "print('Results from KNN(1):', H_sota_1)\n",
    "print('Results from KNN(3):', H_sota_3)\n",
    "print('Results from KNN(5):', H_sota_5)\n",
    "print('Results from KNN(10):', H_sota_10)\n",
    "print('Results from KNN(20):', H_sota_20)\n",
    "print(' Ground truth:', H_true) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "deep_TE20190813.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
